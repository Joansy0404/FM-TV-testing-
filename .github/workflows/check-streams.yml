name: Check M3U Streams (Smart & Fast)

on:
  workflow_dispatch:
  schedule:
    - cron: '0 */6 * * *'

permissions:
  contents: write

jobs:
  check-streams:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests m3u8 concurrent.futures

      - name: Check Streams
        run: |
          python - << 'EOF'
          import requests
          import os
          from datetime import datetime
          import re
          import time
          from concurrent.futures import ThreadPoolExecutor, as_completed
          from requests.adapters import HTTPAdapter
          from urllib3.util.retry import Retry
          
          # Create a session with connection pooling and retries
          session = requests.Session()
          retry_strategy = Retry(
              total=1,
              backoff_factor=0.1,
              status_forcelist=[429, 500, 502, 503, 504],
          )
          adapter = HTTPAdapter(pool_connections=20, pool_maxsize=20, max_retries=retry_strategy)
          session.mount("http://", adapter)
          session.mount("https://", adapter)
          
          def get_runner_location():
              try:
                  ip_info = requests.get('https://ipinfo.io/json', timeout=3).json()
                  return {
                      "country": ip_info.get('country', 'US'),
                      "city": ip_info.get('city', 'Unknown'),
                  }
              except:
                  return {"country": "US", "city": "Unknown"}
          
          # Pre-computed geo mapping for speed
          GEO_MAPPING = {
              'UK': ['United Kingdom', 'Ireland'],
              'US': ['United States', 'Canada'],
              'USA': ['United States', 'Canada'], 
              'CA': ['Canada', 'United States'],
              'AU': ['Australia', 'New Zealand'],
              'AUS': ['Australia', 'New Zealand'],
              'AE': ['UAE', 'Saudi Arabia', 'Qatar'],
              'UAE': ['UAE', 'Saudi Arabia', 'Qatar'],
              'AR': ['Argentina', 'Chile', 'Uruguay'],
              'IE': ['Ireland', 'United Kingdom'],
              'ZA': ['South Africa'],
              'YE': ['Yemen', 'Saudi Arabia'],
              'NZ': ['New Zealand', 'Australia']
          }
          
          def extract_country_fast(group_name):
              """Fast country extraction using pre-computed mapping"""
              group_upper = group_name.upper()
              
              # Check exact matches first (fastest)
              if group_upper in GEO_MAPPING:
                  return group_upper
              
              # Check if it's a 2-letter country code
              if len(group_name) == 2 and group_name.isalpha():
                  return group_name.upper()
              
              # Quick substring check for common patterns
              for country in ['UK', 'USA', 'CANADA', 'AUSTRALIA', 'UAE']:
                  if country in group_upper:
                      return country if country in GEO_MAPPING else country[:2]
              
              return 'UNKNOWN'
          
          def check_stream_fast(stream_data):
              """Fast stream check with minimal processing"""
              name, url, group = stream_data
              
              try:
                  headers = {
                      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                      'Accept': '*/*'
                  }
                  
                  # Single HEAD request with short timeout
                  response = session.head(url, timeout=2, headers=headers, allow_redirects=True)
                  
                  if response.status_code < 400:
                      return {
                          'name': name, 'group': group, 'url': url,
                          'status': 'working', 'code': response.status_code, 'error': None
                      }
                  
                  # Fast error categorization
                  if response.status_code == 403:
                      country = extract_country_fast(group)
                      regions = GEO_MAPPING.get(country, [country])
                      return {
                          'name': name, 'group': group, 'url': url,
                          'status': 'access_denied', 'code': 403, 
                          'error': 'Geo-blocked', 'regions': regions, 'country': country
                      }
                  elif response.status_code == 404:
                      return {
                          'name': name, 'group': group, 'url': url,
                          'status': 'not_found', 'code': 404, 'error': 'Not found'
                      }
                  else:
                      return {
                          'name': name, 'group': group, 'url': url,
                          'status': 'http_error', 'code': response.status_code, 
                          'error': f'HTTP {response.status_code}'
                      }
                      
              except requests.exceptions.Timeout:
                  return {
                      'name': name, 'group': group, 'url': url,
                      'status': 'timeout', 'code': None, 'error': 'Timeout'
                  }
              except Exception as e:
                  error_str = str(e)
                  if "resolve" in error_str.lower():
                      status = 'dns_error'
                      error = 'DNS failed'
                  else:
                      status = 'connection_error' 
                      error = 'Connection failed'
                  
                  return {
                      'name': name, 'group': group, 'url': url,
                      'status': status, 'code': None, 'error': error
                  }
          
          def parse_m3u_fast(file_path):
              """Fast M3U parsing with efficient filtering"""
              if not os.path.exists(file_path):
                  return []
                  
              try:
                  with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                      content = f.read()
              except:
                  return []
              
              streams = []
              lines = content.split('\n')
              
              # Pre-compile regex for speed
              extinf_pattern = re.compile(r'#EXTINF:.*?group-title="([^"]*)".*?,(.+)')
              
              for i, line in enumerate(lines):
                  if line.startswith('#EXTINF'):
                      match = extinf_pattern.search(line)
                      if match and i + 1 < len(lines):
                          group = match.group(1) or "No Group"
                          name = match.group(2).strip() or "Unknown"
                          url = lines[i + 1].strip()
                          
                          if url and not url.startswith('#') and 'http' in url:
                              # Fast filtering - skip obvious movies/series
                              name_lower = name.lower()
                              group_lower = group.lower()
                              
                              # Quick exclusion check
                              if not any(x in name_lower or x in group_lower 
                                       for x in ['movie', 'film', 'serie', '(20', '(19']):
                                  # Skip video file extensions
                                  if not url.endswith(('.mkv', '.mp4', '.avi')):
                                      streams.append((name, url, group))
              
              return streams
          
          # Main execution
          runner_loc = get_runner_location()
          
          streams = parse_m3u_fast('channel playlist.m3u')
          total_streams = len(streams)
          print(f"Found {total_streams} channel streams to check")
          
          # Process streams in parallel batches
          results = []
          batch_size = 50  # Process in batches to avoid overwhelming servers
          
          for i in range(0, len(streams), batch_size):
              batch = streams[i:i+batch_size]
              print(f"Processing batch {i//batch_size + 1}/{(len(streams)-1)//batch_size + 1} ({len(batch)} streams)")
              
              with ThreadPoolExecutor(max_workers=10) as executor:
                  batch_results = list(executor.map(check_stream_fast, batch))
                  results.extend(batch_results)
              
              # Small delay between batches
              time.sleep(0.5)
          
          # Categorize results
          working_streams = 0
          failures = {
              'access_denied': [], 'timeout': [], 'dns_error': [], 'not_found': [],
              'server_error': [], 'connection_error': [], 'http_error': []
          }
          
          for result in results:
              if result['status'] == 'working':
                  working_streams += 1
              else:
                  category = result['status']
                  if category in failures:
                      failures[category].append(result)
          
          total_failures = sum(len(f) for f in failures.values())
          working_percentage = (working_streams / total_streams * 100) if total_streams > 0 else 0
          
          # Generate optimized report
          report = f"""# 📺 M3U Stream Status Report

**Generated on:** {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC  
**GitHub Actions Runner Location:** {runner_loc['country']} ({runner_loc['city']})

## 📊 Summary

| Metric | Count | Percentage |
|--------|-------|------------|
| **Total Streams** | {total_streams} | 100% |
| **✅ Working Streams** | {working_streams} | {working_percentage:.1f}% |
| **❌ Failed Streams** | {total_failures} | {100-working_percentage:.1f}% |

## 📁 Files Processed
- `channel playlist.m3u`: {total_streams} streams (movies/series filtered out)

"""
          
          if total_failures > 0:
              report += "## 📋 Failure Analysis\n\n"
              
              failure_titles = {
                  'access_denied': '🚫 Geo-Blocked Streams',
                  'timeout': '⏱️ Connection Timeouts', 
                  'dns_error': '🌐 DNS Failures',
                  'not_found': '❓ Not Found (404)',
                  'connection_error': '🔗 Connection Errors',
                  'http_error': '🌍 Other HTTP Errors'
              }
              
              for category, streams_list in failures.items():
                  if streams_list:
                      title = failure_titles.get(category, category.replace('_', ' ').title())
                      report += f"### {title} ({len(streams_list)} streams)\n\n"
                      
                      if category == 'access_denied':
                          report += "| Channel | Group | Likely Working Regions | Code |\n"
                          report += "|---------|-------|----------------------|------|\n"
                          for stream in streams_list[:20]:  # Limit to first 20 for readability
                              regions = ', '.join(stream.get('regions', ['Unknown']))
                              name = stream['name'].replace('|', '\\|')[:30]
                              group = stream['group'].replace('|', '\\|')[:15]
                              report += f"| {name} | {group} | {regions} | {stream['code']} |\n"
                      else:
                          report += "| Channel | Group | Error | Code |\n"
                          report += "|---------|-------|-------|------|\n"
                          for stream in streams_list[:15]:  # Limit for readability
                              name = stream['name'].replace('|', '\\|')[:30]
                              group = stream['group'].replace('|', '\\|')[:15] 
                              error = stream['error']
                              code = stream['code'] or 'N/A'
                              report += f"| {name} | {group} | {error} | {code} |\n"
                      
                      report += "\n"
          
          report += f"""## 📈 Geographic Notes

- Tests run from GitHub Actions ({runner_loc['country']})
- Geo-blocked streams show likely working regions
- {len(failures.get('access_denied', []))} streams appear geo-restricted
- DNS/timeout errors may indicate server issues

---
*Last updated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC*
"""
          
          with open('report.md', 'w', encoding='utf-8') as f:
              f.write(report)
          
          print(f"Complete! {working_streams}/{total_streams} working ({working_percentage:.1f}%)")
          EOF

      - name: Commit and push report
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "github-actions[bot]"
          
          git add report.md
          if git diff --staged --quiet; then
            echo "No changes to commit"
            exit 0
          fi
          
          git stash push -m "temp stash for rebase"
          git pull --rebase origin main
          git stash pop
          
          git add report.md
          git commit -m "📺 Stream check: $(date -u '+%Y-%m-%d %H:%M UTC')"
          
          for i in {1..3}; do
            if git push; then
              echo "Successfully pushed"
              break
            else
              echo "Push failed, retrying..."
              git pull --rebase origin main
              sleep 2
            fi
          done
