name: Check M3U Streams (Enhanced)

on:
  workflow_dispatch:
    inputs:
      check_vods:
        description: 'Check VOD files (URLs ending with #.mkv)'
        required: false
        default: false
        type: boolean
      check_ppv:
        description: 'Check PPV channels (may be offline)'
        required: false
        default: false
        type: boolean
      check_fast:
        description: 'Check FAST channels (Free Ad-Supported TV)'
        required: false
        default: false
        type: boolean
      timeout_seconds:
        description: 'Timeout per stream (seconds)'
        required: false
        default: '10'
        type: string
      batch_size:
        description: 'Streams per batch (lower = slower but more reliable)'
        required: false
        default: '5'
        type: string
      base_delay:
        description: 'Seconds between requests (higher = less rate limiting)'
        required: false
        default: '2.0'
        type: string
  schedule:
    - cron: '0 */8 * * *'  # Runs every 8 hours

permissions:
  contents: write

jobs:
  check-streams:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests m3u8

      - name: Enhanced Stream Check
        env:
          CHECK_VODS: ${{ github.event.inputs.check_vods || 'false' }}
          CHECK_PPV: ${{ github.event.inputs.check_ppv || 'false' }}
          CHECK_FAST: ${{ github.event.inputs.check_fast || 'false' }}
          TIMEOUT_SECONDS: ${{ github.event.inputs.timeout_seconds || '10' }}
          BATCH_SIZE: ${{ github.event.inputs.batch_size || '5' }}
          BASE_DELAY: ${{ github.event.inputs.base_delay || '2.0' }}
        run: |
          set -e
          python - << 'EOF'
          import requests
          import os
          from datetime import datetime, timezone
          import re
          import time
          import random
          
          # Configuration from environment
          CHECK_VODS = os.getenv('CHECK_VODS', 'false').lower() == 'true'
          CHECK_PPV = os.getenv('CHECK_PPV', 'false').lower() == 'true' 
          CHECK_FAST = os.getenv('CHECK_FAST', 'false').lower() == 'true'
          TIMEOUT_SECONDS = int(os.getenv('TIMEOUT_SECONDS', '10'))
          BATCH_SIZE = int(os.getenv('BATCH_SIZE', '5'))
          BASE_DELAY = float(os.getenv('BASE_DELAY', '2.0'))
          MAX_RETRIES = 3
          
          print(f"Enhanced Configuration: VODs={CHECK_VODS}, PPV={CHECK_PPV}, FAST={CHECK_FAST}")
          print(f"Batch={BATCH_SIZE}, Delay={BASE_DELAY}s, Timeout={TIMEOUT_SECONDS}s")
          
          def get_geo_headers(group_name):
              """Get region-appropriate headers based on stream group"""
              group_lower = group_name.lower()
              
              base_headers = {
                  'Accept': '*/*',
                  'Accept-Encoding': 'gzip, deflate, br',
                  'Connection': 'keep-alive',
                  'Cache-Control': 'no-cache',
                  'Sec-Fetch-Dest': 'video',
                  'Sec-Fetch-Mode': 'no-cors',
                  'Sec-Fetch-Site': 'cross-site'
              }
              
              if any(x in group_lower for x in ['uk', 'britain', 'bbc', 'itv', 'sky']):
                  base_headers.update({
                      'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                      'Accept-Language': 'en-GB,en;q=0.9',
                      'Referer': 'https://www.google.co.uk/'
                  })
              elif any(x in group_lower for x in ['canada', 'ca ', 'cbc', 'ctv']):
                  base_headers.update({
                      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                      'Accept-Language': 'en-CA,en;q=0.9,fr-CA;q=0.8',
                      'Referer': 'https://www.google.ca/'
                  })
              elif any(x in group_lower for x in ['australia', 'aus', 'abc australia']):
                  base_headers.update({
                      'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                      'Accept-Language': 'en-AU,en;q=0.9',
                      'Referer': 'https://www.google.com.au/'
                  })
              else:  # Default to US
                  base_headers.update({
                      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                      'Accept-Language': 'en-US,en;q=0.9',
                      'Referer': 'https://www.google.com/'
                  })
              
              return base_headers
          
          def check_stream_with_geo_fallback(url, timeout, stream_type, group_name, retry_count=0):
              """Enhanced stream check with rate limiting and geo header fallback"""
              
              # Rate limiting delay with jitter
              jitter = random.uniform(0.2, 0.8)
              time.sleep(BASE_DELAY + jitter)
              
              try:
                  # Get appropriate headers for the stream's likely region
                  headers = get_geo_headers(group_name)
                  
                  # Adjust timeout for PPV
                  if stream_type == 'PPV':
                      timeout = min(timeout * 2, 30)
                  
                  response = requests.get(url, timeout=timeout, headers=headers, stream=True, allow_redirects=True)
                  
                  # Handle rate limiting
                  if response.status_code == 429:
                      if retry_count < MAX_RETRIES:
                          backoff_time = (2 ** retry_count) * BASE_DELAY + random.uniform(1, 3)
                          print(f"    Rate limited, backing off for {backoff_time:.1f}s")
                          time.sleep(backoff_time)
                          return check_stream_with_geo_fallback(url, timeout, stream_type, group_name, retry_count + 1)
                      else:
                          return {"status": "rate_limited", "code": 429, "error": "Rate limited after retries"}
                  
                  # Handle geo-blocking - try alternative headers
                  elif response.status_code == 403:
                      if retry_count < 2:
                          print(f"    Geo-blocked, trying alternative headers...")
                          
                          fallback_headers = [
                              {  # US headers
                                  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                                  'Accept-Language': 'en-US,en;q=0.9',
                                  'Accept': '*/*'
                              },
                              {  # UK headers  
                                  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                                  'Accept-Language': 'en-GB,en;q=0.9',
                                  'Accept': '*/*'
                              }
                          ]
                          
                          time.sleep(1)
                          alt_response = requests.get(
                              url, 
                              timeout=timeout, 
                              headers=fallback_headers[retry_count], 
                              stream=True
                          )
                          
                          if alt_response.status_code < 400:
                              region_note = "US" if retry_count == 0 else "UK"
                              return {
                                  "status": "working", 
                                  "code": alt_response.status_code, 
                                  "note": f"Works with {region_note} headers"
                              }
                          else:
                              return check_stream_with_geo_fallback(url, timeout, stream_type, group_name, retry_count + 1)
                      else:
                          return {"status": "geo_blocked", "code": 403, "error": "Blocked with all header combinations"}
                  
                  # Success case
                  elif response.status_code < 400:
                      # Test if stream has content (for live streams)
                      if stream_type != 'VOD':
                          try:
                              chunk = next(response.iter_content(chunk_size=1024), None)
                              if not chunk:
                                  return {"status": "empty_stream", "code": response.status_code, "error": "No content"}
                          except:
                              pass
                      
                      return {"status": "working", "code": response.status_code, "error": None, "type": stream_type}
                  
                  # Other HTTP errors
                  else:
                      if response.status_code == 404:
                          error_msg = "Stream not found"
                          if stream_type == 'PPV':
                              error_msg += " (PPV may be offline)"
                          return {"status": "not_found", "code": 404, "error": error_msg}
                      elif response.status_code in [500, 502, 503, 504]:
                          return {"status": "server_error", "code": response.status_code, "error": f"Server error ({response.status_code})"}
                      else:
                          return {"status": "http_error", "code": response.status_code, "error": f"HTTP {response.status_code}"}
              
              except requests.exceptions.Timeout:
                  error_msg = f"Timeout after {timeout}s"
                  if stream_type == 'PPV':
                      error_msg += " (PPV may be preparing)"
                  return {"status": "timeout", "code": None, "error": error_msg}
              except requests.exceptions.ConnectionError as e:
                  if "refused" in str(e).lower():
                      return {"status": "connection_refused", "code": None, "error": "Connection refused"}
                  else:
                      return {"status": "connection_error", "code": None, "error": "Network error"}
              except Exception as e:
                  return {"status": "unknown_error", "code": None, "error": str(e)[:100]}
          
          def is_vod_url(url):
              return url.endswith('#.mkv')
          
          def is_fast_channel(name, group):
              return 'fast' in group.lower()
          
          def is_ppv_channel(name, group):
              group_lower = group.lower()
              name_lower = name.lower()
              event_groups = ['uk events', 'usa events', 'events']
              ppv_indicators = ['ppv', 'pay per view', 'boxing', 'ufc', 'wwe']
              
              return (any(event_group in group_lower for event_group in event_groups) or
                      any(indicator in name_lower for indicator in ppv_indicators) or 
                      any(indicator in group_lower for indicator in ppv_indicators))
          
          def classify_stream_type(stream):
              url = stream['url']
              name = stream['name']
              group = stream['group']
              
              if is_vod_url(url):
                  return 'VOD'
              elif is_ppv_channel(name, group):
                  return 'PPV'
              elif is_fast_channel(name, group):
                  return 'FAST'
              else:
                  return 'Channel'
          
          def get_stream_emoji(stream_type):
              emojis = {
                  'VOD': 'Movies',
                  'PPV': 'PPV', 
                  'FAST': 'FAST',
                  'Channel': 'Channel'
              }
              return emojis.get(stream_type, 'Unknown')
          
          def should_skip_stream(stream):
              stream_type = classify_stream_type(stream)
              
              if stream_type == 'VOD' and not CHECK_VODS:
                  return True, f"VOD file (skipped)"
              elif stream_type == 'PPV' and not CHECK_PPV:
                  return True, f"PPV channel (skipped)"
              elif stream_type == 'FAST' and not CHECK_FAST:
                  return True, f"FAST channel (skipped)"
              
              return False, None
          
          def parse_m3u(file_path):
              if not os.path.exists(file_path):
                  print(f"File not found: {file_path}")
                  return []
                  
              streams = []
              
              try:
                  with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                      lines = f.readlines()
              except Exception as e:
                  print(f"Error reading {file_path}: {str(e)}")
                  return []
                  
              for i in range(len(lines)):
                  line = lines[i].strip()
                  if line.startswith('#EXTINF'):
                      try:
                          group_match = re.search(r'group-title="([^"]*)"', line)
                          group = group_match.group(1) if group_match else "No Group"
                          
                          name_part = line.split(',')[-1].strip()
                          name = name_part if name_part else "Unknown Channel"
                          
                          if i + 1 < len(lines):
                              url = lines[i + 1].strip()
                              if url and not url.startswith('#') and ('http' in url or 'rtmp' in url):
                                  streams.append({
                                      'name': name,
                                      'url': url,
                                      'group': group
                                  })
                      except Exception as e:
                          print(f"Error parsing line {i}: {str(e)}")
                          continue
              
              return streams
          
          # Initialize counters
          total_streams = 0
          checked_streams = 0
          working_streams = 0
          skipped_streams = 0
          
          # Enhanced failure categorization
          categorized_failures = {
              'rate_limited': [],
              'geo_blocked': [],
              'connection_refused': [],
              'not_found': [],
              'server_error': [],
              'timeout': [],
              'connection_error': [],
              'unknown_error': []
          }
          
          skipped_categories = {
              'vods': [],
              'ppv': [],
              'fast': []
          }
          
          # M3U files to process
          m3u_files = ['channel playlist.m3u']
          
          print("Starting enhanced stream check...")
          start_time = datetime.now(timezone.utc)
          
          for m3u_file in m3u_files:
              print(f"\nProcessing {m3u_file}...")
              
              try:
                  streams = parse_m3u(m3u_file)
                  file_total = len(streams)
                  total_streams += file_total
                  
                  print(f"Found {file_total} total streams in {m3u_file}")
                  
                  # Filter streams to check
                  streams_to_check = []
                  for stream in streams:
                      should_skip, skip_reason = should_skip_stream(stream)
                      if should_skip:
                          skipped_streams += 1
                          stream_type = classify_stream_type(stream)
                          print(f"Skipped: {stream['name'][:40]}... [{get_stream_emoji(stream_type)}] - {skip_reason}")
                          
                          # Categorize skipped streams
                          if "VOD" in skip_reason:
                              skipped_categories['vods'].append({
                                  'name': stream['name'],
                                  'group': stream['group'],
                                  'file': m3u_file
                              })
                          elif "PPV" in skip_reason:
                              skipped_categories['ppv'].append({
                                  'name': stream['name'],
                                  'group': stream['group'],
                                  'file': m3u_file
                              })
                          elif "FAST" in skip_reason:
                              skipped_categories['fast'].append({
                                  'name': stream['name'],
                                  'group': stream['group'],
                                  'file': m3u_file
                              })
                      else:
                          streams_to_check.append(stream)
                  
                  # Process streams in batches
                  for i in range(0, len(streams_to_check), BATCH_SIZE):
                      batch = streams_to_check[i:i + BATCH_SIZE]
                      batch_num = i // BATCH_SIZE + 1
                      
                      print(f"\nProcessing batch {batch_num} ({len(batch)} streams)")
                      batch_working = 0
                      
                      for stream in batch:
                          checked_streams += 1
                          stream_type = classify_stream_type(stream)
                          stream_emoji = get_stream_emoji(stream_type)
                          
                          print(f"Checking {checked_streams}: {stream['name'][:40]}... [{stream_emoji}]")
                          
                          result = check_stream_with_geo_fallback(
                              stream['url'], 
                              TIMEOUT_SECONDS, 
                              stream_type, 
                              stream['group']
                          )
                          
                          if result['status'] == 'working':
                              working_streams += 1
                              batch_working += 1
                              note = f" ({result['note']})" if 'note' in result else ""
                              print(f"  Working [{stream_emoji}]{note}")
                          else:
                              print(f"  Failed [{stream_emoji}]: {result['error']}")
                              
                              # Categorize the failure
                              failure_info = {
                                  'name': stream['name'],
                                  'group': stream['group'],
                                  'type': stream_type,
                                  'error': result['error'],
                                  'code': result.get('code'),
                                  'file': m3u_file
                              }
                              
                              if result['status'] in categorized_failures:
                                  categorized_failures[result['status']].append(failure_info)
                              else:
                                  categorized_failures['unknown_error'].append(failure_info)
                      
                      # Adaptive delay between batches
                      success_rate = batch_working / len(batch) if batch else 0
                      if success_rate < 0.3:
                          delay = BASE_DELAY * 3
                      elif success_rate < 0.6:
                          delay = BASE_DELAY * 2
                      else:
                          delay = BASE_DELAY
                      
                      print(f"Batch {batch_num}: {batch_working}/{len(batch)} working ({success_rate:.1%})")
                      
                      if i + BATCH_SIZE < len(streams_to_check):
                          print(f"Waiting {delay:.1f}s before next batch...")
                          time.sleep(delay)
                  
              except Exception as e:
                  print(f"Error processing {m3u_file}: {str(e)}")
                  continue
          
          # Calculate final statistics
          total_checked = checked_streams
          total_failures = sum(len(failures) for failures in categorized_failures.values())
          working_percentage = (working_streams / total_checked * 100) if total_checked > 0 else 0
          end_time = datetime.now(timezone.utc)
          duration = end_time - start_time
          
          print(f"\nCheck completed in {duration.total_seconds():.1f} seconds")
          print(f"Results: {working_streams} working, {total_failures} failed, {skipped_streams} skipped")
          print(f"Success rate: {working_percentage:.1f}%")
          
          if total_streams == 0:
              print("No streams found to process!")
              import sys
              sys.exit(1)
          
          print("Stream checking completed successfully")
          
          # Generate enhanced report
          print("Generating report...")
          
          try:
              checked_percentage = (total_checked/total_streams*100) if total_streams > 0 else 0
              failure_percentage = (total_failures/total_checked*100) if total_checked > 0 else 0
              skipped_percentage = (skipped_streams/total_streams*100) if total_streams > 0 else 0
              
              vod_config = 'Enabled' if CHECK_VODS else 'Disabled'
              ppv_config = 'Enabled' if CHECK_PPV else 'Disabled'
              fast_config = 'Enabled' if CHECK_FAST else 'Disabled'
              
              report = "# M3U Stream Status Report (Enhanced)\n\n"
              report += f"**Generated on:** {end_time.strftime('%Y-%m-%d %H:%M:%S')} UTC  \n"
              report += f"**Duration:** {duration.total_seconds():.1f} seconds  \n"
              report += f"**Configuration:** VODs: {vod_config}, PPV: {ppv_config}, FAST: {fast_config}  \n"
              report += f"**Rate Limiting:** {BATCH_SIZE} streams per batch, {BASE_DELAY}s base delay\n\n"
              
              # Summary section
              report += "## Summary\n\n"
              report += "| Metric | Count | Percentage |\n"
              report += "|--------|-------|-----------|\n"
              report += f"| **Total Streams Found** | {total_streams} | 100.0% |\n"
              report += f"| **Checked Streams** | {total_checked} | {checked_percentage:.1f}% |\n"
              report += f"| **Working Streams** | {working_streams} | {working_percentage:.1f}% |\n"
              report += f"| **Failed Streams** | {total_failures} | {failure_percentage:.1f}% |\n"
              report += f"| **Skipped Streams** | {skipped_streams} | {skipped_percentage:.1f}% |\n\n"
              
              # Failure Analysis section
              if total_failures > 0:
                  report += f"## Failure Analysis ({total_failures} total failures)\n\n"
                  
                  failure_order = [
                      ('rate_limited', 'Rate Limited', 'Server limiting request frequency'),
                      ('geo_blocked', 'Geo-blocked', 'Blocked from all tested regions'),
                      ('connection_refused', 'Connection Refused', 'Server actively refusing connections'),
                      ('not_found', 'Not Found (404)', 'Stream URL no longer exists'),
                      ('server_error', 'Server Errors', 'Server-side technical issues'),
                      ('timeout', 'Connection Timeouts', 'Server slow/overloaded'),
                      ('connection_error', 'Connection Errors', 'Network connectivity issues'),
                      ('unknown_error', 'Unknown Errors', 'Unexpected errors')
                  ]
                  
                  for category, title, description in failure_order:
                      failures = categorized_failures.get(category, [])
                      if failures:
                          report += f"### {title} ({len(failures)} streams)\n"
                          report += f"*{description}*\n\n"
                          report += "| Channel | Group | Type | Error | Code | File |\n"
                          report += "|---------|-------|------|-------|------|---------|\n"
                          
                          for stream in failures[:20]:  # Limit to prevent huge reports
                              try:
                                  name = str(stream.get('name', 'Unknown'))[:30].replace('|', '\\|')
                                  group = str(stream.get('group', 'No Group'))[:20].replace('|', '\\|')
                                  stream_type = str(stream.get('type', 'Channel'))
                                  error = str(stream.get('error', 'Unknown'))[:40].replace('|', '\\|')
                                  code = str(stream.get('code', 'N/A'))
                                  file_name = str(stream.get('file', 'Unknown')).replace('|', '\\|')
                                  
                                  report += f"| {name} | {group} | {stream_type} | {error} | {code} | {file_name} |\n"
                              except Exception as e:
                                  continue
                          
                          if len(failures) > 20:
                              report += f"*... and {len(failures) - 20} more*\n"
                          report += "\n"
              
              # Skipped Streams section
              if skipped_streams > 0:
                  report += f"## Skipped Streams ({skipped_streams} total)\n\n"
                  
                  # PPV/Event Channels
                  if skipped_categories['ppv'] and not CHECK_PPV:
                      ppv_groups = {}
                      for item in skipped_categories['ppv']:
                          group = item.get('group', 'No Group')
                          ppv_groups[group] = ppv_groups.get(group, 0) + 1
                      
                      report += f"### PPV/Event Channels ({len(skipped_categories['ppv'])} skipped)\n"
                      report += "*Enable \"Check PPV\" in workflow dispatch to test these*\n\n"
                      report += "| Group | Count |\n"
                      report += "|-------|---------|\n"
                      
                      for group, count in sorted(ppv_groups.items()):
                          group_safe = str(group).replace('|', '\\|')
                          report += f"| {group_safe} | {count} |\n"
                      report += "\n"
                  
                  # FAST Channels
                  if skipped_categories['fast'] and not CHECK_FAST:
                      fast_groups = {}
                      for item in skipped_categories['fast']:
                          group = item.get('group', 'No Group')
                          fast_groups[group] = fast_groups.get(group, 0) + 1
                      
                      report += f"### FAST Channels ({len(skipped_categories['fast'])} skipped)\n"
                      report += "*Enable \"Check FAST\" in workflow dispatch to test these*\n\n"
                      report += "| Group | Count |\n"
                      report += "|-------|---------|\n"
                      
                      for group, count in sorted(fast_groups.items()):
                          group_safe = str(group).replace('|', '\\|')
                          report += f"| {group_safe} | {count} |\n"
                      report += "\n"
                  
                  # VOD Files
                  if skipped_categories['vods'] and not CHECK_VODS:
                      vod_groups = {}
                      for item in skipped_categories['vods']:
                          group = item.get('group', 'No Group')
                          vod_groups[group] = vod_groups.get(group, 0) + 1
                      
                      report += f"### VOD Files ({len(skipped_categories['vods'])} skipped)\n"
                      report += "*Enable \"Check VODs\" in workflow dispatch to test these*\n\n"
                      report += "| Group | Count |\n"
                      report += "|-------|---------|\n"
                      
                      for group, count in sorted(vod_groups.items()):
                          group_safe = str(group).replace('|', '\\|')
                          report += f"| {group_safe} | {count} |\n"
                      report += "\n"
              
              # Configuration Notes
              report += f"## Enhanced Configuration Notes\n\n"
              report += f"- **VOD Checking:** {vod_config} - URLs ending with #.mkv are VOD files\n"
              report += f"- **PPV/Event Checking:** {ppv_config} - UK EVENTS & USA EVENTS groups automatically detected\n"
              report += f"- **FAST Checking:** {fast_config} - Groups containing 'FAST' automatically detected\n"
              report += f"- **Geographic Headers:** Automatic region detection from group names with fallback headers\n"
              report += f"- **Rate Limiting:** {BATCH_SIZE} streams per batch with {BASE_DELAY}s delays and exponential backoff\n"
              report += f"- **Timeout:** {TIMEOUT_SECONDS} seconds per stream (PPV channels get 2x timeout)\n\n"
              
              report += "## Manual Testing Options\n\n"
              report += "To test specific content types:\n"
              report += "1. Go to **Actions** → **Check M3U Streams (Enhanced)** → **Run workflow**\n"
              report += "2. Toggle **Check VODs** to test video-on-demand content\n"
              report += "3. Toggle **Check PPV** to test Pay-Per-View channels\n"
              report += "4. Toggle **Check FAST** to test Free Ad-Supported TV channels\n"
              report += "5. Adjust **Batch Size** (lower = slower but more reliable)\n"
              report += "6. Adjust **Base Delay** (higher = less rate limiting)\n\n"
              
              report += "## Enhanced Features\n\n"
              report += "- **Rate Limiting Protection:** Exponential backoff for 429 errors\n"
              report += "- **Geo-aware Headers:** Region-appropriate headers based on stream groups\n"
              report += "- **Fallback Headers:** Try US/UK headers for geo-blocked content\n"
              report += "- **Batch Processing:** Configurable batch sizes with adaptive delays\n"
              report += "- **Random Jitter:** Prevents synchronized requests\n"
              report += "- **Enhanced Error Categorization:** Better failure analysis\n\n"
              
              report += "---\n"
              report += f"*Last updated: {end_time.strftime('%Y-%m-%d %H:%M:%S')} UTC*\n"
              report += "*Enhanced report with rate limiting and geo-header testing*\n"
              
              print("Report generation completed successfully")
              
          except Exception as e:
              print(f"Error generating report: {str(e)}")
              import traceback
              traceback.print_exc()
              import sys
              sys.exit(1)
          
          # Write report
          print("Writing report to file...")
          try:
              with open('report.md', 'w', encoding='utf-8') as f:
                  f.write(report)
              print("Enhanced report generated successfully!")
              print(f"Final Summary: {working_streams}/{total_checked} streams working ({working_percentage:.1f}%), {skipped_streams} skipped")
              
              # Verify file was written
              if os.path.exists('report.md'):
                  file_size = os.path.getsize('report.md')
                  print(f"Report file created: {file_size} bytes")
              else:
                  print("Report file was not created!")
                  import sys
                  sys.exit(1)
              
          except Exception as e:
              print(f"Error writing report: {str(e)}")
              import traceback
              traceback.print_exc()
              import sys
              sys.exit(1)
          
          print("Script completed successfully!")
          EOF
          
          # Verify Python script succeeded
          if [ $? -eq 0 ]; then
            echo "Python script completed successfully"
          else
            echo "Python script failed with exit code $?"
            exit 1
          fi
          
          # Double-check report file exists
          if [ -f "report.md" ]; then
            echo "Report file confirmed"
            echo "Report size: $(wc -c < report.md) bytes"
            echo "First few lines of report:"
            head -5 report.md
          else
            echo "Report file missing after Python execution!"
            ls -la
            exit 1
          fi

      - name: Commit and push report
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "github-actions[bot]"
          
          # Check if report was generated
          if [[ -f "report.md" ]]; then
            echo "Report file found"
            ls -la report.md
          else
            echo "Report file not found!"
            ls -la
            exit 1
          fi
          
          # Pull latest changes first to avoid conflicts
          echo "Pulling latest changes..."
          git pull origin main || {
            echo "Pull failed, trying to resolve conflicts..."
            git reset --hard HEAD
            git pull origin main
          }
          
          git add report.md
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            # Enhanced commit message
            COMMIT_MSG="Enhanced stream report"
            if [[ "${CHECK_VODS:-false}" == "true" ]]; then
              COMMIT_MSG="$COMMIT_MSG +VODs"
            fi
            if [[ "${CHECK_PPV:-false}" == "true" ]]; then
              COMMIT_MSG="$COMMIT_MSG +PPV" 
            fi
            if [[ "${CHECK_FAST:-false}" == "true" ]]; then
              COMMIT_MSG="$COMMIT_MSG +FAST" 
            fi
            COMMIT_MSG="$COMMIT_MSG - $(date -u '+%Y-%m-%d %H:%M UTC')"
            
            echo "Committing: $COMMIT_MSG"
            git commit -m "$COMMIT_MSG"
            
            # Push with retry logic
            echo "Pushing changes..."
            for i in {1..3}; do
              if git push origin main; then
                echo "Report pushed successfully"
                break
              else
                echo "Push attempt $i failed, retrying..."
                if [ $i -lt 3 ]; then
                  sleep 2
                  git pull --rebase origin main
                else
                  echo "Failed to push after 3 attempts"
                  exit 1
                fi
              fi
            done
          fi
