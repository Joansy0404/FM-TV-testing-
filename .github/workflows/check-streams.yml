name: Enhanced Hot-Swap Stream Checker

on:
  workflow_dispatch:
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours

permissions:
  contents: write

jobs:
  check-streams:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests m3u8 fuzzywuzzy python-levenshtein

      - name: Run Enhanced Stream Checker
        run: |
          python - << 'EOF'
          import requests, os, re, time
          from datetime import datetime
          from fuzzywuzzy import fuzz
          from collections import defaultdict

          # --- CONFIG ---
          BACKUP_SOURCES = [
            {'url': 'https://github.com/Jehovah-witnesses-here/Stream-collection-/raw/refs/heads/main/moj.m3u',
             'name': 'moj', 'priority': 1, 'reliability_score': 0.85, 'network': 'generic'},
            {'url': 'https://github.com/Jehovah-witnesses-here/Stream-collection-/raw/refs/heads/main/dlive.m3u',
             'name': 'dlive', 'priority': 2, 'reliability_score': 0.75, 'network': 'generic'}
          ]
          M3U_FILES = ['vod playlist.m3u', 'channel playlist.m3u']
          TARGET_FILE = 'channel playlist.m3u'

          CATEGORY_MAP = {
            'news':  ['news', 'cnn', 'bbc', 'al jazeera', 'sky news', 'fox news'],
            'sports':['sport', 'espn', 'sky sports', 'bein', 'nba', 'fifa'],
            'movies':['cinema', 'movie', 'film', 'hbo', 'starz'],
            'kids':  ['kids', 'cartoon', 'disney', 'spacetoon'],
            'music': ['music', 'mtv', 'vh1'],
            'entertainment': ['comedy', 'drama', 'talk show']
          }
          
          LANGUAGE_MAP = {
            'arabic': ['arabic', 'ÿπÿ±ÿ®Ÿä', 'mbc', 'al jazeera'],
            'english': ['english', 'cnn', 'bbc', 'sky', 'hbo'],
            'french': ['french', 'france', 'canal']
          }
          
          QUALITY_MAP = {
            'hd': ['hd', '720p', '1080p'],
            '4k': ['4k', 'uhd'],
            'sd': ['sd', '480p']
          }
          
          NETWORK_PATTERNS = ['bbc', 'mbc', 'sky', 'bein', 'cnn', 'disney']

          def get_category(name):
              name = name.lower()
              for k, vals in CATEGORY_MAP.items():
                  if any(v in name for v in vals):
                      return k
              return 'other'

          def get_language(name):
              name = name.lower()
              for k, vals in LANGUAGE_MAP.items():
                  if any(v in name for v in vals):
                      return k
              return 'unknown'

          def get_quality(name):
              name = name.lower()
              for k, vals in QUALITY_MAP.items():
                  if any(v in name for v in vals):
                      return k
              return 'unknown'

          def get_network(name):
              name = name.lower()
              for n in NETWORK_PATTERNS:
                  if n in name:
                      return n
              return 'generic'

          def download_backups():
              all_streams = {}
              for src in BACKUP_SOURCES:
                  try:
                      print(f"Downloading backup source: {src['name']}...")
                      r = requests.get(src['url'], timeout=30)
                      lines = r.text.splitlines()
                      count = 0
                      for i, line in enumerate(lines):
                          if line.startswith('#EXTINF'):
                              name = line.split(',')[-1].strip()
                              url = lines[i+1].strip() if i+1 < len(lines) else ''
                              if name and url.startswith('http'):
                                  meta = {
                                      'url': url,
                                      'category': get_category(name),
                                      'lang': get_language(name),
                                      'quality': get_quality(name),
                                      'network': get_network(name),
                                      'source': src['name'],
                                      'priority': src['priority'],
                                      'reliability': src['reliability_score']
                                  }
                                  if name not in all_streams:
                                      all_streams[name] = []
                                  all_streams[name].append(meta)
                                  count += 1
                      print(f"  Found {count} streams in {src['name']}")
                  except Exception as e:
                      print(f"Error downloading {src['url']}: {e}")
              print(f"Total backup channels loaded: {len(all_streams)}")
              return all_streams

          backup_streams = download_backups()

          def parse_m3u(file):
              streams = []
              if not os.path.exists(file): return [], []
              with open(file, encoding='utf-8', errors='ignore') as f:
                  lines = f.readlines()
              for i, line in enumerate(lines):
                  if line.startswith('#EXTINF'):
                      name = line.split(',')[-1].strip()
                      url = lines[i+1].strip() if i+1 < len(lines) else ''
                      streams.append({
                          'name': name, 'url': url, 'extinf': i, 'url_line': i+1,
                          'category': get_category(name),
                          'lang': get_language(name),
                          'quality': get_quality(name),
                          'network': get_network(name)
                      })
              return streams, lines

          def check_stream(url):
              try:
                  r = requests.head(url, timeout=15)
                  if r.status_code < 400:
                      return True
              except:
                  pass
              try:
                  r = requests.get(url, timeout=15, stream=True)
                  if r.status_code < 400:
                      return True
              except:
                  pass
              return False

          def score_candidate(candidate, original, health):
              score = 0
              if candidate['quality'] == original['quality']:
                  score += 25
              if candidate['category'] == original['category']:
                  score += 30
              if candidate['lang'] == original['lang']:
                  score += 15
              if candidate['network'] == original['network']:
                  score += 10
              score += fuzz.ratio(candidate['url'], original['url']) // 8
              score += int(candidate.get('reliability', 0.5) * 20)
              key = f"{candidate['source']}:{candidate['url'][:40]}"
              h = health.get(key, {'attempts':0,'success':0})
              attempts = h.get('attempts', 1)
              succ = h.get('success', 0)
              hist = (succ / attempts) if attempts else 0
              score += int(hist * 10)
              return score

          def find_backup(original, used_urls, health, swap_attempts):
              candidates = []
              # First pass: exact category and language match
              for name, backups in backup_streams.items():
                  for b in backups:
                      if b['url'] in used_urls: continue
                      if b['category'] == original['category'] and b['lang'] == original['lang']:
                          candidates.append((b, name, score_candidate(b, original, health)))
              # Second pass: category OR language match
              if not candidates:
                  for name, backups in backup_streams.items():
                      for b in backups:
                          if b['url'] in used_urls: continue
                          if b['category'] == original['category'] or b['lang'] == original['lang']:
                              candidates.append((b, name, score_candidate(b, original, health)-10))
              # Third pass: any available backup
              if not candidates:
                  for name, backups in backup_streams.items():
                      for b in backups:
                          if b['url'] in used_urls: continue
                          candidates.append((b, name, score_candidate(b, original, health)-20))
              
              if not candidates:
                  return None, None
              
              candidates.sort(key=lambda x: x[2], reverse=True)
              for cand, backup_name, sc in candidates[:3]:
                  attempt = {
                      "url": cand['url'],
                      "backup_name": backup_name,
                      "source": cand['source'],
                      "score": sc,
                      "category": cand['category'],
                      "lang": cand['lang'],
                      "quality": cand['quality'],
                      "network": cand['network'],
                      "result": None
                  }
                  if check_stream(cand['url']):
                      attempt["result"] = "working"
                      swap_attempts.append(attempt)
                      return cand, backup_name
                  else:
                      attempt["result"] = "fail"
                      swap_attempts.append(attempt)
              return None, None

          # --- MAIN LOOP ---
          total, fixed, failed = 0, 0, 0
          used_backup_urls = set()
          health = defaultdict(lambda: {'attempts':0,'success':0,'fails':0})
          swap_log = []
          swap_attempts = []
          failures = []

          for m3u in M3U_FILES:
              print(f"\nProcessing {m3u}...")
              streams, lines = parse_m3u(m3u)
              for s in streams:
                  total += 1
                  print(f"Checking {s['name'][:40]}...", end='')
                  if check_stream(s['url']):
                      key = f"{s['name']}:{s['url'][:40]}"
                      health[key]['attempts'] += 1
                      health[key]['success'] += 1
                      print("‚úÖ")
                      continue
                  print("‚ùå Trying backup...", end='')
                  if m3u == TARGET_FILE:
                      backup, backup_name = find_backup(s, used_backup_urls, health, swap_attempts)
                      if backup:
                          key = f"{backup['source']}:{backup['url'][:40]}"
                          health[key]['attempts'] += 1
                          health[key]['success'] += 1
                          lines[s['url_line']] = backup['url']+'\n'
                          used_backup_urls.add(backup['url'])
                          fixed += 1
                          swap_log.append({
                              "original": s['name'],
                              "backup_channel": backup_name,
                              "old_url": s['url'],
                              "new_url": backup['url'],
                              "source": backup['source'],
                              "category": backup['category'],
                              "lang": backup['lang'],
                              "quality": backup['quality'],
                              "network": backup['network']
                          })
                          print(f"Swapped! ({backup['source']}: '{backup_name}')")
                          continue
                  key = f"{s['name']}:{s['url'][:40]}"
                  health[key]['attempts'] += 1
                  health[key]['fails'] += 1
                  failed += 1
                  failures.append({
                      "name": s['name'],
                      "url": s['url'],
                      "category": s['category'],
                      "lang": s['lang'],
                      "quality": s['quality'],
                      "network": s['network']
                  })
                  print("FAILED")
              if m3u == TARGET_FILE:
                  with open(m3u, 'w', encoding='utf-8') as f:
                      f.writelines(lines)

          # --- REPORT GENERATION ---
          with open("report.md", "w", encoding="utf-8") as f:
              f.write(f"# üöÄ Enhanced Stream Checker Report\n")
              f.write(f"**Date:** {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC\n\n")
              f.write(f"**Total Streams Checked:** {total}\n")
              f.write(f"**Fixed via Hot-Swap:** {fixed}\n")
              f.write(f"**Still Failing:** {failed}\n\n")
              
              if swap_log:
                  f.write("## üîÑ Channel Replacements Applied\n")
                  f.write("*Shows which channels were replaced with which backup channels:*\n\n")
                  f.write("| # | Original Channel (FAILED) | ‚ûú | Backup Channel (WORKING) | Source | Category | Language | Quality |\n")
                  f.write("|---|---------------------------|---|--------------------------|--------|----------|----------|----------|\n")
                  for i, s in enumerate(swap_log, 1):
                      f.write(f"| {i} | `{s['original']}` | ‚ûú | `{s['backup_channel']}` | {s['source']} | {s['category']} | {s['lang']} | {s['quality']} |\n")
                  
                  f.write("\n### üìã Detailed URL Changes\n")
                  f.write("*Complete URL mappings for technical reference:*\n\n")
                  for i, s in enumerate(swap_log, 1):
                      f.write(f"#### {i}. {s['original']} ‚ûú {s['backup_channel']}\n")
                      f.write(f"**Original URL (FAILED):** `{s['old_url']}`\n\n")
                      f.write(f"**Replacement URL (WORKING):** `{s['new_url']}`\n\n")
                      f.write("---\n\n")
              
              if swap_attempts:
                  f.write("## üß† All Swap Attempts (Including Failed)\n")
                  f.write("*Shows all backup URLs that were tested during the swap process:*\n\n")
                  f.write("| Attempt | Backup Channel | Backup URL | Source | Score | Category | Language | Quality | Network | Result |\n")
                  f.write("|---------|----------------|------------|--------|-------|----------|----------|---------|---------|--------|\n")
                  for i, s in enumerate(swap_attempts, 1):
                      status_emoji = "‚úÖ" if s['result'] == "working" else "‚ùå"
                      backup_name = s.get('backup_name', 'Unknown')
                      f.write(f"| {i} | `{backup_name}` | `{s['url'][:50]}{'...' if len(s['url']) > 50 else ''}` | {s['source']} | {s['score']} | {s['category']} | {s['lang']} | {s['quality']} | {s['network']} | {status_emoji} {s['result']} |\n")
              
              if failures:
                  f.write("\n## ‚ùå Streams Still Failing\n")
                  f.write("*These streams are still broken and need manual attention:*\n\n")
                  for i, s in enumerate(failures, 1):
                      f.write(f"### {i}. {s['name']}\n")
                      f.write(f"**Failed URL:** `{s['url']}`\n\n")
                      f.write(f"**Category:** {s['category']} | **Language:** {s['lang']} | **Quality:** {s['quality']} | **Network:** {s['network']}\n\n")
                      f.write("---\n\n")
              
              f.write("## üìä Session Statistics\n")
              f.write("*Performance metrics for this checking session:*\n\n")
              f.write("| Metric | Count | Percentage |\n")
              f.write("|--------|-------|------------|\n")
              f.write(f"| Total Streams | {total} | 100% |\n")
              f.write(f"| Working Streams | {total - failed - fixed} | {((total - failed - fixed) / total * 100):.1f}% |\n")
              f.write(f"| Fixed via Swap | {fixed} | {(fixed / total * 100):.1f}% |\n")
              f.write(f"| Still Failing | {failed} | {(failed / total * 100):.1f}% |\n")
              
              if health:
                  f.write(f"\n## üè• Backup Source Health\n")
                  f.write("*Reliability statistics for backup sources used in this session:*\n\n")
                  f.write("| Source/Stream | Attempts | Success | Fails | Success Rate |\n")
                  f.write("|---------------|----------|---------|-------|-------------|\n")
                  for k, v in sorted(health.items(), key=lambda x: x[1]['success'], reverse=True):
                      success_rate = (v['success'] / v['attempts'] * 100) if v['attempts'] > 0 else 0
                      f.write(f"| `{k[:40]}{'...' if len(k) > 40 else ''}` | {v['attempts']} | {v['success']} | {v['fails']} | {success_rate:.1f}% |\n")

          print(f"\nDone! Total: {total} | Fixed: {fixed} | Failed: {failed}")
          print(f"Report generated: report.md")
          if swap_log:
              print(f"‚úÖ Successfully replaced {len(swap_log)} failing channels:")
              for s in swap_log:
                  print(f"  ‚Ä¢ '{s['original']}' ‚ûú '{s['backup_channel']}' (from {s['source']})")
          if failures:
              print(f"‚ùå {len(failures)} channels still need manual attention")
          EOF

      - name: Commit and push changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "github-actions[bot]"
          git add .
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "üß† Enhanced Hot-Swap: Fixed failing streams with intelligent backup and channel name tracking in report.md"
            git push
          fi
