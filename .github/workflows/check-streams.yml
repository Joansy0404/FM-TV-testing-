name: Enhanced Hot-Swap Stream Checker

on:
  workflow_dispatch:
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours

permissions:
  contents: write

jobs:
  check-streams:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests m3u8 fuzzywuzzy python-levenshtein

      - name: Run Enhanced Stream Checker
        run: |
          python - <<'EOF'
import requests, os, re, time
from datetime import datetime
from fuzzywuzzy import fuzz
from collections import defaultdict

# --- CONFIG ---
BACKUP_SOURCES = [
  {'url': 'https://github.com/Jehovah-witnesses-here/Stream-collection-/raw/refs/heads/main/moj.m3u',
   'name': 'moj', 'priority': 1, 'reliability_score': 0.85, 'network': 'generic'},
  {'url': 'https://github.com/Jehovah-witnesses-here/Stream-collection-/raw/refs/heads/main/dlive.m3u',
   'name': 'dlive', 'priority': 2, 'reliability_score': 0.75, 'network': 'generic'}
]
M3U_FILES = ['vod playlist.m3u', 'channel playlist.m3u']
TARGET_FILE = 'channel playlist.m3u'

CATEGORY_MAP = {
  'news':  ['news', 'cnn', 'bbc', 'al jazeera', 'sky news', 'fox news'],
  'sports':['sport', 'espn', 'sky sports', 'bein', 'nba', 'fifa'],
  'movies':['cinema', 'movie', 'film', 'hbo', 'starz'],
  'kids':  ['kids', 'cartoon', 'disney', 'spacetoon'],
  'music': ['music', 'mtv', 'vh1'],
  'entertainment': ['comedy', 'drama', 'talk show'],
}
LANGUAGE_MAP = {
  'arabic': ['arabic', 'Ø¹Ø±Ø¨ÙŠ', 'mbc', 'al jazeera'],
  'english': ['english', 'cnn', 'bbc', 'sky', 'hbo'],
  'french': ['french', 'france', 'canal'],
}
QUALITY_MAP = {
  'hd': ['hd', '720p', '1080p'],
  '4k': ['4k', 'uhd'],
  'sd': ['sd', '480p']
}
NETWORK_PATTERNS = [
  'bbc', 'mbc', 'sky', 'bein', 'cnn', 'disney'
]

def get_category(name):
    name = name.lower()
    for k, vals in CATEGORY_MAP.items():
        if any(v in name for v in vals):
            return k
    return 'other'
def get_language(name):
    name = name.lower()
    for k, vals in LANGUAGE_MAP.items():
        if any(v in name for v in vals):
            return k
    return 'unknown'
def get_quality(name):
    name = name.lower()
    for k, vals in QUALITY_MAP.items():
        if any(v in name for v in vals):
            return k
    return 'unknown'
def get_network(name):
    name = name.lower()
    for n in NETWORK_PATTERNS:
        if n in name:
            return n
    return 'generic'

def download_backups():
    all_streams = {}
    for src in BACKUP_SOURCES:
        try:
            r = requests.get(src['url'], timeout=30)
            lines = r.text.splitlines()
            for i, line in enumerate(lines):
                if line.startswith('#EXTINF'):
                    name = line.split(',')[-1].strip()
                    url = lines[i+1].strip() if i+1 < len(lines) else ''
                    if name and url.startswith('http'):
                        meta = {
                            'url': url,
                            'category': get_category(name),
                            'lang': get_language(name),
                            'quality': get_quality(name),
                            'network': get_network(name),
                            'source': src['name'],
                            'priority': src['priority'],
                            'reliability': src['reliability_score']
                        }
                        if name not in all_streams:
                            all_streams[name] = []
                        all_streams[name].append(meta)
        except Exception as e:
            print(f"Error downloading {src['url']}: {e}")
    return all_streams

backup_streams = download_backups()

def parse_m3u(file):
    streams = []
    if not os.path.exists(file): return [], []
    with open(file, encoding='utf-8', errors='ignore') as f:
        lines = f.readlines()
    for i, line in enumerate(lines):
        if line.startswith('#EXTINF'):
            name = line.split(',')[-1].strip()
            url = lines[i+1].strip() if i+1 < len(lines) else ''
            streams.append({
                'name': name, 'url': url, 'extinf': i, 'url_line': i+1,
                'category': get_category(name),
                'lang': get_language(name),
                'quality': get_quality(name),
                'network': get_network(name),
            })
    return streams, lines

def check_stream(url):
    try:
        r = requests.head(url, timeout=15)
        if r.status_code < 400:
            return True
    except:
        pass
    try:
        r = requests.get(url, timeout=15, stream=True)
        if r.status_code < 400:
            return True
    except:
        pass
    return False

def score_candidate(candidate, original, health):
    score = 0
    if candidate['quality'] == original['quality']:
        score += 25
    if candidate['category'] == original['category']:
        score += 30
    if candidate['lang'] == original['lang']:
        score += 15
    if candidate['network'] == original['network']:
        score += 10
    score += fuzz.ratio(candidate['url'], original['url']) // 8
    score += int(candidate.get('reliability', 0.5) * 20)
    key = f"{candidate['source']}:{candidate['url'][:40]}"
    h = health.get(key, {'attempts':0,'success':0})
    attempts = h.get('attempts', 1)
    succ = h.get('success', 0)
    hist = (succ / attempts) if attempts else 0
    score += int(hist * 10)
    return score

def find_backup(original, used_urls, health, swap_attempts):
    candidates = []
    for name, backups in backup_streams.items():
        for b in backups:
            if b['url'] in used_urls: continue
            if b['category'] == original['category'] and b['lang'] == original['lang']:
                candidates.append((b, score_candidate(b, original, health)))
    if not candidates:
        for name, backups in backup_streams.items():
            for b in backups:
                if b['url'] in used_urls: continue
                if b['category'] == original['category'] or b['lang'] == original['lang']:
                    candidates.append((b, score_candidate(b, original, health)-10))
    if not candidates:
        return None
    candidates.sort(key=lambda x: x[1], reverse=True)
    for cand, sc in candidates[:3]:
        attempt = {
            "url": cand['url'],
            "source": cand['source'],
            "score": sc,
            "category": cand['category'],
            "lang": cand['lang'],
            "quality": cand['quality'],
            "network": cand['network'],
            "result": None
        }
        if check_stream(cand['url']):
            attempt["result"] = "working"
            swap_attempts.append(attempt)
            return cand
        else:
            attempt["result"] = "fail"
            swap_attempts.append(attempt)
    return None

# --- MAIN LOOP ---
total, fixed, failed = 0, 0, 0
used_backup_urls = set()
health = defaultdict(lambda: {'attempts':0,'success':0,'fails':0})
swap_log = []
swap_attempts = []
failures = []

for m3u in M3U_FILES:
    print(f"\nProcessing {m3u}...")
    streams, lines = parse_m3u(m3u)
    for s in streams:
        total += 1
        print(f"Checking {s['name'][:40]}...", end='')
        if check_stream(s['url']):
            key = f"{s['name']}:{s['url'][:40]}"
            health[key]['attempts'] += 1
            health[key]['success'] += 1
            print("âœ…")
            continue
        print("âŒ Trying backup...", end='')
        if m3u == TARGET_FILE:
            backup = find_backup(s, used_backup_urls, health, swap_attempts)
            if backup:
                key = f"{backup['source']}:{backup['url'][:40]}"
                health[key]['attempts'] += 1
                health[key]['success'] += 1
                lines[s['url_line']] = backup['url']+'\n'
                used_backup_urls.add(backup['url'])
                fixed += 1
                swap_log.append({
                    "original": s['name'],
                    "old_url": s['url'],
                    "new_url": backup['url'],
                    "source": backup['source'],
                    "category": backup['category'],
                    "lang": backup['lang'],
                    "quality": backup['quality'],
                    "network": backup['network']
                })
                print(f"Swapped! ({backup['source']})")
                continue
        key = f"{s['name']}:{s['url'][:40]}"
        health[key]['attempts'] += 1
        health[key]['fails'] += 1
        failed += 1
        failures.append({
            "name": s['name'],
            "url": s['url'],
            "category": s['category'],
            "lang": s['lang'],
            "quality": s['quality'],
            "network": s['network']
        })
        print("FAILED")
    if m3u == TARGET_FILE:
        with open(m3u, 'w', encoding='utf-8') as f:
            f.writelines(lines)

# --- REPORT GENERATION ---
with open("report.md", "w", encoding="utf-8") as f:
    f.write(f"# ðŸš€ Enhanced Stream Checker Report\n")
    f.write(f"**Date:** {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC\n\n")
    f.write(f"**Total Streams Checked:** {total}\n")
    f.write(f"**Fixed via Hot-Swap:** {fixed}\n")
    f.write(f"**Still Failing:** {failed}\n\n")
    if swap_log:
        f.write("## ðŸ”„ Hot-Swaps Applied\n")
        f.write("| Channel | Old URL | New URL | Backup Source | Category | Lang | Quality | Network |\n")
        f.write("|---|---|---|---|---|---|---|---|\n")
        for s in swap_log:
            f.write(f"| {s['original'][:30]} | {s['old_url'][:12]}... | {s['new_url'][:12]}... | {s['source']} | {s['category']} | {s['lang']} | {s['quality']} | {s['network']} |\n")
    if swap_attempts:
        f.write("\n## ðŸ§  Swap Attempts (Top 10)\n")
        f.write("| URL | Source | Score | Cat | Lang | Qual | Net | Result |\n")
        f.write("|---|---|---|---|---|---|---|---|\n")
        for s in swap_attempts[:10]:
            f.write(f"| {s['url'][:12]}... | {s['source']} | {s['score']} | {s['category']} | {s['lang']} | {s['quality']} | {s['network']} | {s['result']} |\n")
    if failures:
        f.write("\n## âŒ Remaining Failures (Top 10)\n")
        f.write("| Name | URL | Cat | Lang | Qual | Net |\n")
        f.write("|---|---|---|---|---|---|\n")
        for s in failures[:10]:
            f.write(f"| {s['name'][:25]} | {s['url'][:12]}... | {s['category']} | {s['lang']} | {s['quality']} | {s['network']} |\n")
    f.write("\n## ðŸ“Š Backup Source Health (Session Only)\n")
    f.write("| Key | Attempts | Success | Fails |\n")
    f.write("|---|---|---|---|\n")
    for k, v in health.items():
        f.write(f"| {k} | {v['attempts']} | {v['success']} | {v['fails']} |\n")

print(f"\nDone! Total: {total} | Fixed: {fixed} | Failed: {failed}")
EOF

      - name: Commit and push changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "github-actions[bot]"
          git add .
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "ðŸ§  Enhanced Hot-Swap: Fixed failing streams with intelligent backup and included health in report.md"
            git push
