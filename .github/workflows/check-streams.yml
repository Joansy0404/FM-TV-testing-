name: Enhanced Hot-Swap Stream Checker

on:
  workflow_dispatch:
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours

permissions:
  contents: write

jobs:
  check-streams:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests m3u8 fuzzywuzzy python-levenshtein

      - name: Run Enhanced Stream Checker
        run: |
          python - << 'EOF'
          import requests, os, re, json, time
          from datetime import datetime, timedelta
          from fuzzywuzzy import fuzz
          from collections import defaultdict

          # --- CONFIG ---
          BACKUP_SOURCES = [
            {'url': 'https://github.com/Jehovah-witnesses-here/Stream-collection-/raw/refs/heads/main/moj.m3u',
             'name': 'moj', 'priority': 1, 'reliability_score': 0.85, 'network': 'generic'},
            {'url': 'https://github.com/Jehovah-witnesses-here/Stream-collection-/raw/refs/heads/main/dlive.m3u',
             'name': 'dlive', 'priority': 2, 'reliability_score': 0.75, 'network': 'generic'}
          ]
          M3U_FILES = ['vod playlist.m3u', 'channel playlist.m3u']
          TARGET_FILE = 'channel playlist.m3u'
          # --- END CONFIG ---

          # --- ENHANCEMENT TABLES ---
          CATEGORY_MAP = {
            'news':  ['news', 'cnn', 'bbc', 'al jazeera', 'sky news', 'fox news'],
            'sports':['sport', 'espn', 'sky sports', 'bein', 'nba', 'fifa'],
            'movies':['cinema', 'movie', 'film', 'hbo', 'starz'],
            'kids':  ['kids', 'cartoon', 'disney', 'spacetoon'],
            'music': ['music', 'mtv', 'vh1'],
            'entertainment': ['comedy', 'drama', 'talk show'],
          }
          LANGUAGE_MAP = {
            'arabic': ['arabic', 'Ø¹Ø±Ø¨ÙŠ', 'mbc', 'al jazeera'],
            'english': ['english', 'cnn', 'bbc', 'sky', 'hbo'],
            'french': ['french', 'france', 'canal'],
          }
          QUALITY_MAP = {
            'hd': ['hd', '720p', '1080p'],
            '4k': ['4k', 'uhd'],
            'sd': ['sd', '480p']
          }
          NETWORK_PATTERNS = [
            'bbc', 'mbc', 'sky', 'bein', 'cnn', 'disney'
          ]

          # --- UTILS ---
          def get_category(name):
              name = name.lower()
              for k, vals in CATEGORY_MAP.items():
                  if any(v in name for v in vals):
                      return k
              return 'other'
          def get_language(name):
              name = name.lower()
              for k, vals in LANGUAGE_MAP.items():
                  if any(v in name for v in vals):
                      return k
              return 'unknown'
          def get_quality(name):
              name = name.lower()
              for k, vals in QUALITY_MAP.items():
                  if any(v in name for v in vals):
                      return k
              return 'unknown'
          def get_network(name):
              name = name.lower()
              for n in NETWORK_PATTERNS:
                  if n in name:
                      return n
              return 'generic'

          # --- HEALTH TRACKING ---
          health_file = 'backup_health_scores.json'
          def load_health():
              if os.path.exists(health_file):
                  try:
                      with open(health_file, 'r') as f:
                          return json.load(f)
                  except Exception as e:
                      print(f"Error loading health data: {e}")
              return {}
          def save_health(data):
              try:
                  with open(health_file, 'w') as f:
                      json.dump(data, f)
              except Exception as e:
                  print(f"Error saving health data: {e}")

          health_data = load_health()

          def update_health(source, url, success):
              key = f"{source}:{url[:40]}"
              if key not in health_data:
                  health_data[key] = {'attempts': 0, 'success': 0, 'fails': 0}
              health_data[key]['attempts'] += 1
              if success:
                  health_data[key]['success'] += 1
              else:
                  health_data[key]['fails'] += 1

          # --- BACKUP DOWNLOADER ---
          def download_backups():
              all_streams = {}
              for src in BACKUP_SOURCES:
                  try:
                      r = requests.get(src['url'], timeout=30)
                      lines = r.text.splitlines()
                      for i, line in enumerate(lines):
                          if line.startswith('#EXTINF'):
                              name = line.split(',')[-1].strip()
                              url = lines[i+1].strip() if i+1 < len(lines) else ''
                              if name and url.startswith('http'):
                                  meta = {
                                      'url': url,
                                      'category': get_category(name),
                                      'lang': get_language(name),
                                      'quality': get_quality(name),
                                      'network': get_network(name),
                                      'source': src['name'],
                                      'priority': src['priority'],
                                      'reliability': src['reliability_score']
                                  }
                                  if name not in all_streams:
                                      all_streams[name] = []
                                  all_streams[name].append(meta)
                  except Exception as e:
                      print(f"Error downloading {src['url']}: {e}")
              return all_streams

          backup_streams = download_backups()

          # --- MAIN CHECKER ---
          def parse_m3u(file):
              streams = []
              if not os.path.exists(file): return []
              with open(file, encoding='utf-8', errors='ignore') as f:
                  lines = f.readlines()
              for i, line in enumerate(lines):
                  if line.startswith('#EXTINF'):
                      name = line.split(',')[-1].strip()
                      url = lines[i+1].strip() if i+1 < len(lines) else ''
                      streams.append({
                          'name': name, 'url': url, 'extinf': i, 'url_line': i+1,
                          'category': get_category(name),
                          'lang': get_language(name),
                          'quality': get_quality(name),
                          'network': get_network(name),
                      })
              return streams, lines

          def check_stream(url):
              try:
                  r = requests.head(url, timeout=15)
                  if r.status_code < 400:
                      return True
              except:
                  pass
              try:
                  r = requests.get(url, timeout=15, stream=True)
                  if r.status_code < 400:
                      return True
              except:
                  pass
              return False

          def score_candidate(candidate, original):
              score = 0
              # Quality matches
              if candidate['quality'] == original['quality']:
                  score += 25
              # Category matches
              if candidate['category'] == original['category']:
                  score += 30
              # Language matches
              if candidate['lang'] == original['lang']:
                  score += 15
              # Network matches
              if candidate['network'] == original['network']:
                  score += 10
              # Fuzzy name match
              score += fuzz.ratio(candidate['url'], original['url']) // 8
              # Reliability
              score += int(candidate.get('reliability', 0.5) * 20)
              # Health
              key = f"{candidate['source']}:{candidate['url'][:40]}"
              health = health_data.get(key, {})
              attempts = health.get('attempts', 1)
              succ = health.get('success', 0)
              hist = (succ / attempts) if attempts else 0
              score += int(hist * 10)
              return score

          def find_backup(original, used_urls):
              candidates = []
              # 1. Name, Category, Quality, Language, Network match
              for name, backups in backup_streams.items():
                  for b in backups:
                      if b['url'] in used_urls: continue
                      # Must match at least category and lang
                      if b['category'] == original['category'] and b['lang'] == original['lang']:
                          candidates.append((b, score_candidate(b, original)))
              # 2. Fallback: relax one criteria
              if not candidates:
                  for name, backups in backup_streams.items():
                      for b in backups:
                          if b['url'] in used_urls: continue
                          if b['category'] == original['category'] or b['lang'] == original['lang']:
                              candidates.append((b, score_candidate(b, original)-10))
              if not candidates:
                  return None
              candidates.sort(key=lambda x: x[1], reverse=True)
              for cand, _ in candidates[:3]:  # Try top 3
                  if check_stream(cand['url']):
                      update_health(cand['source'], cand['url'], True)
                      return cand
                  else:
                      update_health(cand['source'], cand['url'], False)
              return None

          # --- MAIN LOOP ---
          total, fixed, failed = 0, 0, 0
          used_backup_urls = set()
          for m3u in M3U_FILES:
              print(f"\nProcessing {m3u}...")
              streams, lines = parse_m3u(m3u)
              for s in streams:
                  total += 1
                  print(f"Checking {s['name'][:40]}...", end='')
                  if check_stream(s['url']):
                      print("âœ…")
                      continue
                  print("âŒ Trying backup...", end='')
                  if m3u == TARGET_FILE:
                      backup = find_backup(s, used_backup_urls)
                      if backup:
                          lines[s['url_line']] = backup['url']+'\n'
                          used_backup_urls.add(backup['url'])
                          fixed += 1
                          print(f"Swapped! ({backup['source']})")
                          continue
                  failed += 1
                  print("FAILED")
              # Write fixed file
              if m3u == TARGET_FILE:
                  with open(m3u, 'w', encoding='utf-8') as f:
                      f.writelines(lines)

          save_health(health_data)
          print(f"\nDone! Total: {total} | Fixed: {fixed} | Failed: {failed}")
          EOF

      - name: Commit and push changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "github-actions[bot]"
          git add .
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "ðŸ§  Enhanced Hot-Swap: Fixed failing streams with intelligent backup"
            git push
